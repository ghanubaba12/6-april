{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84777178-f034-40eb-9cce-5487d7e6396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the mathematical formula for a linear SVM?\n",
    "ans-\n",
    "A linear Support Vector Machine (SVM) is a type of machine learning algorithm used for binary classification, where the goal is to separate data points into two classes using a hyperplane. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "\n",
    "Given a training dataset with labeled examples {(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}, where x_i represents the input features for the i-th example, and y_i is the corresponding binary label (+1 or -1), the goal of the linear SVM is to find the optimal hyperplane that best separates the data points into their respective classes.\n",
    "\n",
    "The linear SVM seeks to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class, while minimizing the classification error. The hyperplane is defined by the equation:\n",
    "\n",
    "w^T * x + b = 0\n",
    "\n",
    "where w is the normal vector to the hyperplane, b is the bias term, x is the input feature vector, and '^T' denotes the transpose of a vector.\n",
    "\n",
    "The decision function of the linear SVM is given by:\n",
    "\n",
    "f(x) = sign(w^T * x + b)\n",
    "\n",
    "where f(x) is the predicted class label for an input feature vector x, and sign() is the sign function that returns +1 if the argument is positive or zero, and -1 otherwise.\n",
    "\n",
    "The training objective of the linear SVM is to find the optimal values of w and b that maximize the margin while satisfying the following constraints:\n",
    "\n",
    "y_i * (w^T * x_i + b) >= 1\n",
    "\n",
    "for all training examples (x_i, y_i). This constraint ensures that the data points are correctly classified and lie outside the margin.\n",
    "\n",
    "The optimization problem for the linear SVM can be formulated as:\n",
    "\n",
    "minimize: (1/2) ||w||^2\n",
    "\n",
    "subject to: y_i * (w^T * x_i + b) >= 1\n",
    "\n",
    "where ||w||^2 is the L2 norm of the weight vector w, and (1/2) ||w||^2 is a regularization term that encourages a larger margin.\n",
    "\n",
    "The solution to this optimization problem yields the optimal values of w and b, which define the hyperplane that best separates the data points into their respective classes in a linearly separable manner. The training process typically involves solving this optimization problem using techniques such as quadratic programming or gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a36b15-f23e-4b79-8ec7-306c18007866",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "ans-\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that best separates the training data into different classes while maximizing the margin between the classes. The objective function typically used in linear SVM is the hinge loss function, which is combined with a regularization term. The hinge loss function measures the classification error or misclassification of data points, and the regularization term helps in controlling the complexity of the model and preventing overfitting.\n",
    "\n",
    "The standard objective function of a linear SVM can be mathematically represented as:\n",
    "\n",
    "minimize:\n",
    "(1/2) * ||w||^2 + C * Σ max(0, 1 - y_i * (w^T * x_i + b))\n",
    "\n",
    "subject to:\n",
    "y_i * (w^T * x_i + b) ≥ 1 - ξ_i, for i = 1, 2, ..., N\n",
    "\n",
    "where:\n",
    "\n",
    "w is the weight vector of the hyperplane that represents the decision boundary\n",
    "b is the bias term\n",
    "x_i is the i-th training data point\n",
    "y_i is the corresponding label of the i-th training data point (+1 or -1, indicating the class)\n",
    "ξ_i is the slack variable that allows for misclassified points, and ξ_i ≥ 0\n",
    "N is the total number of training data points\n",
    "C is the regularization parameter that controls the trade-off between achieving a larger margin and allowing some misclassifications. A smaller value of C will allow more misclassifications but a larger margin, while a larger value of C will allow fewer misclassifications but a smaller margin.\n",
    "The objective function aims to minimize the regularization term (||w||^2) and the hinge loss (Σ max(0, 1 - y_i * (w^T * x_i + b))), which penalizes misclassified data points. The goal is to find the values of w and b that minimize the objective function, which corresponds to finding the optimal hyperplane that best separates the data into different classes while maximizing the margin between the classes.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33e093-5dce-452d-afd9-d9693dc5fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "ans-The kernel trick is a technique used in Support Vector Machines (SVMs) that allows the SVM to efficiently and effectively handle nonlinearly separable data by implicitly transforming the input features into a higher-dimensional space. This allows the SVM to learn nonlinear decision boundaries without explicitly computing the higher-dimensional feature vectors, which can be computationally expensive.\n",
    "\n",
    "In traditional SVMs, a linear decision boundary (hyperplane) is used to separate data points of different classes. However, many real-world datasets are not linearly separable, and using a linear SVM may result in poor classification performance. The kernel trick addresses this limitation by introducing a kernel function, which computes the similarity (or inner product) between pairs of input feature vectors in the original feature space or a higher-dimensional space.\n",
    "\n",
    "Mathematically, given an input feature vector x, the kernel function K(x, x') computes the similarity between x and another feature vector x' in the original feature space or a higher-dimensional space. The kernel function can be chosen from different types, such as linear, polynomial, Gaussian (RBF), sigmoid, and others, depending on the characteristics of the data and the problem at hand.\n",
    "\n",
    "The key insight of the kernel trick is that the SVM can be formulated entirely in terms of the kernel function, without explicitly computing the transformed feature vectors. This is done by replacing the inner product between feature vectors with the kernel function in the SVM's optimization problem. This allows the SVM to implicitly learn a nonlinear decision boundary in the higher-dimensional space, while avoiding the computational cost of explicitly transforming the feature vectors.\n",
    "\n",
    "The kernel trick has several benefits, including the ability to handle nonlinearly separable data, improved generalization performance, and computational efficiency. It allows SVMs to capture complex patterns and relationships in the data, making them a powerful tool for classification and regression tasks in machine learning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4851a74-24b3-47ce-bc8d-89577b7da1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "ans-\n",
    "In Support Vector Machines (SVMs), support vectors play a crucial role in determining the optimal hyperplane that separates data points into different classes. Support vectors are the data points that lie closest to the decision boundary, also known as the margin. They are the critical data points that influence the positioning of the decision boundary and the margin.\n",
    "\n",
    "The main role of support vectors in SVM can be summarized as follows:\n",
    "\n",
    "Defining the Margin: Support vectors determine the width of the margin, which is the region between the positive and negative class decision boundaries. The margin is maximized by placing the decision boundary as far as possible from the support vectors. The support vectors that lie on the margin are known as the \"margin support vectors\" and are critical in determining the optimal hyperplane.\n",
    "\n",
    "Contributing to the Classifier: Support vectors are used to determine the coefficients of the hyperplane equation. The coefficients are calculated based on the support vectors' positions, and they define the orientation and position of the hyperplane. The support vectors are the data points that have non-zero coefficients in the SVM's decision function, and they are used to classify new data points.\n",
    "\n",
    "Handling Misclassification: Support vectors are also responsible for handling misclassified data points. In a soft-margin SVM, which allows for some misclassification, the support vectors can be misclassified examples. These misclassified support vectors contribute to the formulation of the optimization problem and help in finding a decision boundary that balances the margin and the misclassification error.\n",
    "\n",
    "Here's an example to illustrate the role of support vectors in SVM:\n",
    "\n",
    "Consider a binary classification problem with two-dimensional data points belonging to two classes, labeled as +1 and -1. The goal is to find a decision boundary that separates the two classes. In the example, let's assume that the decision boundary is a linear hyperplane.\n",
    "\n",
    "In the figure below, the blue and red circles represent the data points of the two classes, and the solid line represents the decision boundary found by the SVM.\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "     +1\n",
    "      |\n",
    "      |          Margin\n",
    "      |\n",
    "      |______________________________\n",
    "      |          |                   |\n",
    "      |         +1                  |\n",
    "      |______________________________|   Decision Boundary (Hyperplane)\n",
    "      |         -1                  |\n",
    "      |______________________________|\n",
    "      |\n",
    "      |\n",
    "     -1\n",
    "In this example, the support vectors are the data points that lie on the margin or those that are misclassified. These support vectors are shown as filled circles in the figure, and they determine the position of the decision boundary and the margin. The other data points that are not support vectors do not affect the position of the decision boundary or the margin, as their removal would not change the solution of the SVM.\n",
    "\n",
    "The support vectors are critical for SVMs as they determine the optimal hyperplane that separates the classes and contribute to the classification of new data points. They play a crucial role in defining the margin, contributing to the formulation of the optimization problem, and handling misclassification in SVM.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e816d7-f0b7-4346-8ee5-bfea5c2cf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "ans-Sure! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM) using examples and graphs.\n",
    "\n",
    "Hyperplane:\n",
    "A hyperplane is a decision boundary that separates data points of different classes in SVM. In a two-dimensional feature space (with two input features), a hyperplane is a straight line that separates data points into different classes. In a three-dimensional feature space (with three input features), a hyperplane is a flat plane that separates data points into different classes. The goal of SVM is to find the optimal hyperplane that best separates the data points.\n",
    "Example:\n",
    "Let's consider a simple example of a binary classification problem with two features (x1 and x2) and two classes (positive and negative), as shown in the graph below:\n",
    "\n",
    "lua\n",
    "Copy code\n",
    "   +-----------------+\n",
    "   |     +           |\n",
    "   |       +         |\n",
    "   |         -       |\n",
    "   |           -     |\n",
    "   |             +   |\n",
    "   |               + |\n",
    "   +-----------------+\n",
    "In this example, the positive class is denoted by '+' and the negative class is denoted by '-'. The hyperplane (decision boundary) that separates the two classes is denoted by the solid line.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal plane in SVM refers to the hyperplane that is parallel to the optimal hyperplane (decision boundary) and closest to the data points, but does not touch any data point. The margin is the distance between the optimal hyperplane and the marginal plane. In a binary classification problem, there are two marginal planes, one for each class.\n",
    "Example:\n",
    "In the same example as above, the marginal planes are denoted by the dashed lines that are parallel to the optimal hyperplane and closest to the data points, but do not touch any data point, as shown in the graph below:\n",
    "\n",
    "lua\n",
    "Copy code\n",
    "   +-----------------+\n",
    "   |     +           |\n",
    "   |       +         |\n",
    "   |         -       |\n",
    "   |           -     |\n",
    "   |             +   |\n",
    "   |               + |\n",
    "   +-----------------+\n",
    "         |   |\n",
    "         |---| Marginal Plane (for positive class)\n",
    "             |---| Marginal Plane (for negative class)\n",
    "Soft Margin:\n",
    "In SVM, a soft margin allows for some misclassifications in the training data. The objective is to find a hyperplane that best separates the data points while allowing a certain amount of misclassification. A regularization parameter 'C' is used to control the trade-off between achieving a larger margin and allowing some misclassifications. A smaller value of 'C' allows more misclassifications but a larger margin, while a larger value of 'C' allows fewer misclassifications but a smaller margin.\n",
    "Example:\n",
    "Let's consider an example where the data points are not perfectly separable, as shown in the graph below:\n",
    "\n",
    "lua\n",
    "Copy code\n",
    "   +-----------------+\n",
    "   |     +           |\n",
    "   |       +         |\n",
    "   |         -       |\n",
    "   |           -     |\n",
    "   |             +   |\n",
    "   |               + |\n",
    "   +-----------------+\n",
    "In this case, it may not be possible to find a hyperplane that perfectly separates the data points. However, by allowing for some misclassifications, a hyperplane with a smaller margin can be found, as denoted by the solid line. This is an example of a soft margin SVM, where some data points are allowed to be misclassified.\n",
    "\n",
    "Hard Margin:\n",
    "In contrast to soft margin, a hard margin SVM does not allow for any misclassifications in the training data. It requires that all data points be correctly classified, and the hyperplane perfectly separates the data points without any misclassifications. Hard margin SVM is applicable when the data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0049982-4ee8-4e75-8100-c1d7f645d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "ans-\n",
    "Sure! Here's an implementation of a linear SVM classifier using Python and scikit-learn library on the Iris dataset, along with a comparison of its performance with scikit-learn's implementation.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Select first two features for simplicity\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "C = 1.0  # Regularization parameter\n",
    "svm = SVC(kernel='linear', C=C)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries of the trained model\n",
    "# Based on two features - Sepal Length and Sepal Width\n",
    "h = 0.02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.title('Linear SVM Decision Boundaries (Iris dataset)')\n",
    "plt.show()\n",
    "In the code above, we first load the Iris dataset from the scikit-learn library and select the first two features (Sepal Length and Sepal Width) for simplicity. Then, we split the dataset into a training set and a testing set using the train_test_split function. Next, we train a linear SVM classifier on the training set using the SVC class from scikit-learn, with the kernel parameter set to 'linear' and the regularization parameter C set to 1.0. We then predict the labels for the testing set and compute the accuracy of the model using the accuracy_score function from scikit-learn.\n",
    "\n",
    "Finally, we plot the decision boundaries of the trained model using the contourf function in matplotlib, based on the Sepal Length and Sepal Width features. The filled contour plot shows the decision boundaries, and the scatter plot shows the data points with different colors representing different classes. You can try different values of the regularization parameter C to see how it affects the performance of the model. Higher values of C result in a narrower margin and may lead to overfitting, while lower values of C result in a wider margin but may lead to underfitting. You can experiment with different values of C to find the optimal value for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa2249-d8f2-4be1-8fb0-153bc39e7067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22debeb3-d28a-48eb-8d71-6d322ea5cb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93443a05-9182-482e-9d41-7654395c493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd40760-e3f1-4fe1-879b-e2c8798960d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b6de4-7eb6-402a-b11c-d381d21ad00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
